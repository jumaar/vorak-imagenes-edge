networks:
  vorak-net:
    driver: bridge
    dns:
      - 8.8.8.8  
      - 1.1.1.1  
      
volumes:
  nevera_db:
  nevera_offline_queue:
  nevera_review_queue:
  kiosk_data:
  fridge_status:
  backup_data:
  prometheus_data:

services:
  #--------------------------------------------------------------------------
  # SERVICIO 'NEVERA' - El cerebro del sistema
  #--------------------------------------------------------------------------
  nevera:
    
    build:
      context: .
      dockerfile: ./MODULO-NEVERA/Dockerfile
    image: ghcr.io/jumaar/vorak-imagenes-edge/nevera:latest 
    container_name: vorak-nevera
    restart: unless-stopped
    environment:
      
      - FRIDGE_ID=${FRIDGE_ID}
      - BASE_BACKEND_URL=${BASE_BACKEND_URL}
      - FRIDGE_SECRET=${FRIDGE_SECRET} 
      - CAMERA_DEVICES=/dev/video0,/dev/video2 # IMPORTANTE: Ajusta esto a tus cámaras
      - BAUD_RATE=115200
    volumes:
      # Volumen para la base de datos de productos
      - nevera_db:/app/db
      # Volumen para guardar transacciones si no hay conexión
      - nevera_offline_queue:/app/offline_queue
      # Volumen para guardar imágenes de sesiones de baja confianza
      - nevera_review_queue:/app/review_queue
      # Volumen compartido para comunicar el estado al kiosko
      - fridge_status:/app/status
    devices:
      # --- Acceso al Hardware del Host ---
      # Permite al contenedor acceder a las cámaras
      - "/dev/video0:/dev/video0"
      - "/dev/video2:/dev/video2"
      # Permite al contenedor acceder al ESP32 (ajusta si es necesario)
      - "/dev/ttyUSB0:/dev/ttyUSB0"
    networks:
      - vorak-net

    group_add:
      - "${VIDEO_GID}"
      - "${DIALOUT_GID}"
  

  #--------------------------------------------------------------------------
  # SERVICIO 'KIOSKO' - La interfaz de usuario web
  #--------------------------------------------------------------------------
  kiosko:
    
    build:
      context: .
      dockerfile: ./MODULO-KIOSKO/Dockerfile
    image: ghcr.io/jumaar/vorak-imagenes-edge/kiosko:latest 
    container_name: vorak-kiosko
    restart: unless-stopped
    ports:
      # Expone el puerto del servidor web para que el navegador del host pueda acceder
      - "5000:5000"
      # Expone el puerto para el webhook, que apunta al mismo servidor interno.
      - "9091:5000"
    # --- ¡CORRECCIÓN! ---
    # Usamos 'env_file' para cargar TODAS las variables del .env en el contenedor.
    # Esto asegura que el script de redespliegue tenga acceso a todo lo que necesita.
    env_file: ./.env
    volumes:
      # --- ¡NUEVO! Permisos para controlar Docker ---
      - /var/run/docker.sock:/var/run/docker.sock
      # --- ¡SOLUCIÓN! Montamos el directorio del proyecto para que el kiosko pueda ejecutar git pull en el host.
      - .:/project
      # Volumen para la caché de medios y la playlist
      - kiosk_data:/app/data
      # Volumen compartido para leer el estado de la nevera
      - fridge_status:/app/status
    networks:
      - vorak-net
    group_add:
      - "${DOCKER_GID}"
  #--------------------------------------------------------------------------
  # SERVICIO 'BACKUP' - Realiza copias de seguridad periódicas
  #--------------------------------------------------------------------------
  backup:
    image: alpine:latest
    container_name: vorak-backup
    restart: unless-stopped
    command: >
      /bin/sh -c "
        chmod +x /backup/backup.sh && \
        echo 'Contenedor de backup iniciado. El primer backup se ejecutará en 1 minuto...';
        sleep 60;
        while true; do
          /backup/backup.sh;
          echo 'Backup completado. Próxima ejecución en 24 horas.';
          sleep 86400;
        done
      "
    volumes:
      # Monta el script de backup dentro del contenedor
      - ./MODULO-BACKUP/backup.sh:/backup/backup.sh
      # Monta los volúmenes de origen (solo lectura para seguridad)
      - nevera_offline_queue:/backups/source/offline_queue:ro
      - nevera_review_queue:/backups/source/review_queue:ro
      # Monta el volumen de destino donde se guardarán los backups
      - backup_data:/backups/destination
    networks:
      - vorak-net

  #--------------------------------------------------------------------------
  # STACK DE MONITOREO - Portainer, Prometheus, etc.
  #--------------------------------------------------------------------------
  prometheus:
    # --- ¡CORRECCIÓN! ---
    # Usamos 'build' para crear una imagen personalizada que incluya 'envsubst'.
    build:
      context: ./MODULO-MONITORING
      dockerfile: prometheus.Dockerfile
    image: ghcr.io/jumaar/vorak-imagenes-edge/prometheus:latest
    container_name: vorak-prometheus
    ports:
      # Mapea el puerto 9090 del host al puerto 9090 del contenedor
      - "9090:9090"
    environment:
      # --- ¡CORRECCIÓN! ---
      # Pasamos las variables del .env para que se sustituyan en prometheus.yml
      - GRAFANA_CLOUD_PROMETHEUS_URL=${GRAFANA_CLOUD_PROMETHEUS_URL}
      - GRAFANA_CLOUD_PROMETHEUS_USER=${GRAFANA_CLOUD_PROMETHEUS_USER}
      - GRAFANA_CLOUD_PROMETHEUS_API_KEY=${GRAFANA_CLOUD_PROMETHEUS_API_KEY}
    volumes:
      - ./MODULO-MONITORING/prometheus.yml:/etc/prometheus/prometheus.yml
      - /var/run/docker.sock:/var/run/docker.sock:ro
   
    command:
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    restart: unless-stopped
    networks:
      - vorak-net

  promtail:
    # --- ¡CORRECCIÓN! ---
    # Usamos 'build' para crear una imagen personalizada que incluya 'envsubst'.
    build:
      context: ./MODULO-MONITORING
      dockerfile: promtail.Dockerfile
    image: ghcr.io/jumaar/vorak-imagenes-edge/promtail:latest
    container_name: vorak-promtail
    environment:
      # --- ¡CORRECCIÓN! ---
      # Pasamos las variables del .env para que se sustituyan en promtail.dev.yml
      - GRAFANA_CLOUD_LOKI_URL=${GRAFANA_CLOUD_LOKI_URL}
      - GRAFANA_CLOUD_LOKI_USER=${GRAFANA_CLOUD_LOKI_USER}
      - GRAFANA_CLOUD_LOKI_API_KEY=${GRAFANA_CLOUD_LOKI_API_KEY}
    volumes:
      - ./MODULO-MONITORING/promtail-config.yml:/etc/promtail/config.yml
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    
    restart: unless-stopped
    networks:
      - vorak-net

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.47.2
    container_name: vorak-cadvisor
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:rw
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev:/dev:ro
      - /etc/machine-id:/etc/machine-id:ro
    restart: unless-stopped
    networks:
      - vorak-net
    privileged: true
    command: --docker_only


  node-exporter:
    image: prom/node-exporter:v1.6.1
    container_name: vorak-node-exporter
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--path.rootfs=/rootfs'
      # --- ¡MEJORA! ---
      # Excluimos explícitamente los puntos de montaje de Docker para evitar métricas duplicadas con cAdvisor.
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc|run/docker/netns|var/lib/docker/containers|var/lib/docker/overlay2)($$|/)'
      # --- ¡SOLUCIÓN! Desactivamos el colector NFSd que no es necesario y genera errores. ---
      - '--no-collector.nfsd'
    restart: unless-stopped
    networks:
      - vorak-net


  #--------------------------------------------------------------------------
  # SERVICIO 'DEPLOYER' - Contenedor de utilidad para el redespliegue
  #--------------------------------------------------------------------------
  deployer:
    image: docker:cli
    # --- ¡NUEVO! ---
    # Este servicio no se ejecuta por defecto, solo se usa como plantilla para 'docker compose run'.
    container_name: vorak-deployer
    # Montamos el socket de Docker para que pueda gestionar otros contenedores.
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      # --- ¡SOLUCIÓN! Montamos el mismo directorio del proyecto para que vea los cambios.
      - ${PROJECT_PATH:-.}:/project
    # Cargamos las variables de entorno para que pueda autenticarse en ghcr.io
    env_file: ./.env
    working_dir: /project
